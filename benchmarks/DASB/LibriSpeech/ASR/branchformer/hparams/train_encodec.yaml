# ################################
# Recipe for training an discrete-input ctc ASR system with librispeech.
# Decoding is performed with ctc greedy or LM-rescored decoder.
#
# Authors
# * Pooneh Mousavi 2024
# ################################

# Seed needs to be set at top of yaml, before objects with parameters are made
seed: 1986
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/MP3S-BranchFormer/encodec/<seed>
output_wer_folder: !ref <output_folder>/
save_folder: !ref <output_folder>/save
train_log: !ref <output_folder>/train_log.txt


# Data files
data_folder: data # e,g./path/to/LibriSpeech
# noise/ris dataset will automatically be downloaded
# data_folder_rirs: !ref <data_folder>
train_splits: ["train-clean-100"]
dev_splits: ["dev-clean"]
test_splits: ["test-clean", "test-other"]

skip_prep: False
ckpt_interval_minutes: 25 # save checkpoint every N min
train_csv: !ref <output_folder>/train-clean-100.csv
valid_csv: !ref <output_folder>/dev-clean.csv
test_csv:
   - !ref <output_folder>/test-clean.csv
   - !ref <output_folder>/test-other.csv


####################### Training Parameters ####################################
number_of_epochs: 20
lr: 0.0002
sorting: ascending
precision: fp32

# With data_parallel batch_size is split into N jobs
# With DDP batch_size is multiplied by N jobs
# Must be 3 per GPU to fit 32GB of VRAM
batch_size: 4
test_batch_size: 1

ctc_weight: 0.3
grad_accumulation_factor: 1
max_grad_norm: 5.0
loss_reduction: 'batchmean'
sorting: random
num_workers: 4
precision: fp32 # bf16, fp16 or fp32
avg_checkpoints: 10 # Number of checkpoints to average for evaluation

lr_adam: 0.0008
weight_decay: 0.01

### Config for Tokenizer
# EnCodec parameters
# sample_rate: [24000, 24000, 24000, 24000]
# vocab_size: [1024, 1024, 1024, 1024]
# bandwidth: [1.5, 3.0, 6.0, 12.0, 24.0]
# num_codebooks: [2, 4, 8, 16, 32]
vocab_size: 1024
bandwidth: 1.5
num_codebooks: 2
sample_rate: 24000
# Feature parameters
encoder_dim: 1024
# If set to True, the encoder_dim should be set to the dim of the tokenizer. For encodec it is 128.
init_embedding: False
freeze_embedding: False


# This setup works well for A100 80GB GPU, adapts it to your needs.
# Or turn it off (but training speed will decrease)
dynamic_batching: True
max_batch_length_train: 500
max_batch_length_val: 100 # we reduce it as the beam is much wider (VRAM)
num_bucket: 200
shuffle: True # if true re-creates batches at each epoch shuffling examples.
max_batch_ex: 128
batch_ordering: random
dynamic_batch_sampler_train:
    max_batch_length: !ref <max_batch_length_train>
    num_buckets: !ref <num_bucket>
    shuffle: !ref <shuffle>
    batch_ordering: !ref <batch_ordering>
    max_batch_ex: !ref <max_batch_ex>

dynamic_batch_sampler_valid:
    max_batch_length: !ref <max_batch_length_val>
    num_buckets: !ref <num_bucket>
    shuffle: !ref <shuffle>
    batch_ordering: !ref <batch_ordering>
    max_batch_ex: !ref <max_batch_ex>

# Dataloader options
train_dataloader_opts:
   batch_size: !ref <batch_size>

valid_dataloader_opts:
   batch_size: !ref <batch_size>

test_dataloader_opts:
   batch_size: !ref <test_batch_size>

####################### Model Parameters #######################################

# Transformer
d_model: 512
nhead: 8
num_encoder_layers: 18
num_decoder_layers: 6
csgu_linear_units: 3072
csgu_kernel_size: 31
transformer_dropout: 0.1
activation: !name:torch.nn.GELU
output_neurons: 5000

# Outputs
blank_index: 0
label_smoothing: 0.1
pad_index: 0
bos_index: 1
eos_index: 2

# Decoding parameters
min_decode_ratio: 0.0
max_decode_ratio: 1.0
valid_search_interval: 10
valid_beam_size: 10
test_beam_size: 66
lm_weight: 0.60
ctc_weight_decode: 0.40


test_beam_search:
   beam_size: 143
   topk: 1
   blank_index: !ref <blank_index>
   space_token: ' ' # make sure this is the same as the one used in the tokenizer
   beam_prune_logp: -12.0
   token_prune_min_logp: -1.2
   prune_history: True
   alpha: 0.8
   beta: 1.2
   # can be downloaded from here https://www.openslr.org/11/ or trained with kenLM
   # It can either be a .bin or .arpa ; note: .arpa is much slower at loading
   # If you don't want to use an LM, comment it out or set it to null
   kenlm_model_path: null

# Functions and classes
#
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
   limit: !ref <number_of_epochs>

# EnCodec model (see https://huggingface.co/docs/transformers/v4.31.0/en/model_doc/encodec)
codec: !new:speechbrain.lobes.models.huggingface_transformers.encodec.Encodec
   source: facebook/encodec_24khz  # Only the 24kHz version supports mono audio
   save_path: !ref <save_folder>
   sample_rate: !ref <sample_rate>
   bandwidth: !ref <bandwidth>
   flat_embeddings: False
   freeze: True
   renorm_embeddings: False

discrete_embedding_layer: !new:custom_model.Discrete_EmbeddingLayer
   num_codebooks: !ref <num_codebooks>
   vocab_size: !ref <vocab_size>
   emb_dim: !ref <encoder_dim>
   freeze: !ref <freeze_embedding>
   init: !ref <init_embedding>

attention_mlp: !new:custom_model.AttentionMLP
   input_dim: !ref <encoder_dim>
   hidden_dim: !ref <encoder_dim>


enc: !new:speechbrain.lobes.models.transformer.TransformerASR.TransformerASR # yamllint disable-line rule:line-length
    input_size: !ref <encoder_dim>
    tgt_vocab: !ref <output_neurons>
    d_model: !ref <d_model>
    nhead: !ref <nhead>
    num_encoder_layers: !ref <num_encoder_layers>
    num_decoder_layers: !ref <num_decoder_layers>
    dropout: !ref <transformer_dropout>
    activation: !ref <activation>
    branchformer_activation: !ref <activation>
    encoder_module: branchformer
    csgu_linear_units: !ref <csgu_linear_units>
    kernel_size: !ref <csgu_kernel_size>
    attention_type: RelPosMHAXL
    normalize_before: True
    causal: False


tokenizer: !new:sentencepiece.SentencePieceProcessor

ctc_lin: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <d_model>
    n_neurons: !ref <output_neurons>

seq_lin: !new:speechbrain.nnet.linear.Linear
    input_size: !ref <d_model>
    n_neurons: !ref <output_neurons>

log_softmax: !new:torch.nn.LogSoftmax
    dim: -1

ctc_cost: !name:speechbrain.nnet.losses.ctc_loss
    blank_index: !ref <blank_index>
    reduction: !ref <loss_reduction>

seq_cost: !name:speechbrain.nnet.losses.kldiv_loss
    label_smoothing: !ref <label_smoothing>
    reduction: !ref <loss_reduction>

modules:
   enc: !ref <enc>
   ctc_lin: !ref <ctc_lin>
   attention_mlp: !ref <attention_mlp>
   codec: !ref <codec>
   discrete_embedding_layer: !ref <discrete_embedding_layer>

model: !new:torch.nn.ModuleList
   - [!ref <enc>, !ref <ctc_lin>, !ref <discrete_embedding_layer>, !ref <attention_mlp>]
####################### Decoding & optimiser ###################################

ctc_scorer: !new:speechbrain.decoders.scorer.CTCScorer
    eos_index: !ref <eos_index>
    blank_index: !ref <blank_index>
    ctc_fc: !ref <ctc_lin>


scorer_valid_search: !new:speechbrain.decoders.scorer.ScorerBuilder
    full_scorers: [!ref <ctc_scorer>]
    weights:
        ctc: !ref <ctc_weight_decode>

scorer_test_search: !new:speechbrain.decoders.scorer.ScorerBuilder
    full_scorers: [!ref <ctc_scorer>]
    weights:
        ctc: !ref <ctc_weight_decode>

valid_search: !new:speechbrain.decoders.S2STransformerBeamSearcher
    modules: [!ref <enc>, !ref <seq_lin>]
    bos_index: !ref <bos_index>
    eos_index: !ref <eos_index>
    min_decode_ratio: !ref <min_decode_ratio>
    max_decode_ratio: !ref <max_decode_ratio>
    beam_size: !ref <valid_beam_size>
    using_eos_threshold: False
    length_normalization: True
    scorer: !ref <scorer_valid_search>

test_search: !new:speechbrain.decoders.S2STransformerBeamSearcher
    modules: [!ref <enc>, !ref <seq_lin>]
    bos_index: !ref <bos_index>
    eos_index: !ref <eos_index>
    min_decode_ratio: !ref <min_decode_ratio>
    max_decode_ratio: !ref <max_decode_ratio>
    beam_size: !ref <test_beam_size>
    temperature: 1.15
    using_eos_threshold: False
    length_normalization: True
    scorer: !ref <scorer_test_search>

model_opt_class: !name:torch.optim.AdamW
    lr: !ref <lr_adam>
    betas: (0.9, 0.98)
    eps: 0.000000001
    weight_decay: !ref <weight_decay>

lr_annealing_model: !new:speechbrain.nnet.schedulers.NoamScheduler
    lr_initial: !ref <lr_adam>
    n_warmup_steps: 30000


checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
   checkpoints_dir: !ref <save_folder>
   recoverables:
      model: !ref <model>
      scheduler_model: !ref <lr_annealing_model>
      attention_mlp: !ref <attention_mlp>
      codec: !ref <codec>
      discrete_embedding_layer: !ref <discrete_embedding_layer>
      counter: !ref <epoch_counter>
      tokenizer: !ref <label_encoder>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
   save_file: !ref <train_log>

error_rate_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats

cer_computer: !name:speechbrain.utils.metric_stats.ErrorRateStats
   split_tokens: True
